<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.87.0" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>Summiting mountains using RL&nbsp;&ndash;&nbsp;Nived&#39;s Blog</title><link rel="stylesheet" href="/blog/css/core.min.86e97c9ce79d7ff23aa47166ea466e3ce244928ab4d29a809f1f7b896f214f65fc48475a98961c0ee567475b44b7275c.css" integrity="sha384-hul8nOedf/I6pHFm6kZuPOJEkoq00pqAnx97iW8hT2X8SEdamJYcDuVnR1tEtydc"><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Summiting mountains using RL" /><body><section id="header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/blog/"><span class="site name">Nived's Blog</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="https://nivedwho%2egithub%2eio/"target="_blank" rel="noopener noreferrer">Website</a></nav></div></span></div></section><section id="content"><div class="article-container"><section class="article header">
    <h1 class="article title">Summiting mountains using RL</h1><p class="article date"> </p></section><article class="article markdown-body"><p><strong>March 2021</strong> || Tensorflow solution for the <a href="https://gym.openai.com/envs/MountainCarContinuous-v0/"target="_blank" rel="noopener noreferrer">MountainCarContinuous-v0</a>
 problem</p>
<p><a href="https://colab.research.google.com/github/nivedwho/Colab/blob/main/mountain_car.ipynb"target="_blank" rel="noopener noreferrer"><img  src="https://colab.research.google.com/assets/colab-badge.svg"
        alt="Open in Colab"/></a>
</p>
<h2 id="problem">Problem</h2>
<p>This was the first time I tried solving an OpenAI Gym problem and  this problem looked interesting and was easy to understand.</p>
<p>The problem is about a car placed at the bottom of a hill and our goal is to write a program that helps the car climb the hill. But the car&rsquo;s engine is not powerful enough to do this by itself hence we need to move the car back and forth until it gains sufficient momentum to climb the hill. Lesser the energy consumed, greater will be the score.</p>
<p>The source code for the problem can be seen <a href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/continuous_mountain_car.py"target="_blank" rel="noopener noreferrer">here</a>
</p>
<p>The reward is -1 for each step until the goal position (0.5)is reached. The code should stop after reaching the goal or after 200 iterations.</p>
<h2 id="solution">Solution</h2>
<p>The first thing I did is explore the problem and see how the car is moving. For that all I had to do was to load the mountain car enviornment and call the &ldquo;env.action_space.sample()&rdquo; for getting the actions. This moves the car back and forth with the values specified in its <a href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/continuous_mountain_car.py"target="_blank" rel="noopener noreferrer">source code</a>
.</p>
<p>After 200 iterations code stopped and the car obviously was unable to climb the mountain. More importantly this lets us see the data generated by the car - position, velocity and its relation with the reward.</p>
<p><img  src="https://github.com/nivedwho/Images/blob/main/dumbcar.gif?raw=true"
        alt="Modified Model"/></p>
<p>Before implementing a neural network we need sufficient data from which the car can learn to move correctly. To do this I have implemented the function &ldquo;model_data_preperation&rdquo; that randomly moves the car using the 3 actions- 0(move left), 1(rest) and 2(move right). Instead of setting the reward -1 for all actions, it is set as 1 if the car&rsquo;s position is getting closer to the top of the mountain. Once the score is greater than -198 the data is added to the list of accepted score. Now we have generated the data that tells us what movements are beneficial for us and what all are not. The game is similarly played for 10000 times.</p>
<p>Nextly I have made a sequencial model that learns from the generated data and it is trained.</p>
<p>Again the car is made to move, only this time the actions are set by the trained model and the reward is set like it was at first, ie. always = -1 unless the goal position is reached.</p>
<p><img  src="https://github.com/nivedwho/Images/blob/main/smartcar.gif?raw=true"
        alt="Modified Model"/></p></article><section class="article labels"><a class="category" href=/blog/categories/reinforcement-learning/>Reinforcement Learning</a><a class="category" href=/blog/categories/gym/>Gym</a><a class="category" href=/blog/categories/ml/>ML</a></section>
</div>
<div class="article bottom"><section class="article navigation"><p><a class="link" href="/blog/posts/emotions/"><span class="iconfont icon-article"></span>Sentiment Analysis on Texts</a></p><p><a class="link" href="/blog/posts/stylegan/"><span class="iconfont icon-article"></span>StyleGAN - A very popular GAN</a></p></section></div></section><section id="footer"><div class="footer-wrap">
    <p class="copyright">Nived's Blog</p>
    <p class="powerby"><span>Powered&nbsp;by&nbsp;</span><a href="https://gohugo.io" 
        target="_blank" rel="noopener noreferrer">Hugo</a><span>&nbsp;&amp;&nbsp;</span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank" rel="noopener noreferrer">Notepadium</a></p></div>
</section></body>

</html>