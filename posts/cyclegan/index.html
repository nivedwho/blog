<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.88.1" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>Using GANs to create &#39;art&#39;&nbsp;&ndash;&nbsp;Nived&#39;s Blog</title><link rel="stylesheet" href="/blog/css/core.min.86e97c9ce79d7ff23aa47166ea466e3ce244928ab4d29a809f1f7b896f214f65fc48475a98961c0ee567475b44b7275c.css" integrity="sha384-hul8nOedf/I6pHFm6kZuPOJEkoq00pqAnx97iW8hT2X8SEdamJYcDuVnR1tEtydc"><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Using GANs to create &#39;art&#39;" /><body><section id="header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/blog/"><span class="site name">Nived's Blog</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="https://nivedwho%2egithub%2eio/"target="_blank" rel="noopener noreferrer">Website</a></nav></div></span></div></section><section id="content"><div class="article-container"><section class="article header">
    <h1 class="article title">Using GANs to create 'art'</h1><p class="article date"> </p></section><article class="article markdown-body"><p><strong>December 2020</strong> || A project reproducing the results of the paper <a href="https://arxiv.org/abs/1703.10593"target="_blank" rel="noopener noreferrer">&ldquo;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&rdquo;</a>
 and making some additional changes to improve its performance.</p>
<p><a href="https://colab.research.google.com/github/nivedwho/Colab/blob/main/CycleGAN.ipynb"target="_blank" rel="noopener noreferrer"><img  src="https://colab.research.google.com/assets/colab-badge.svg"
        alt="Open in Colab"/></a>
</p>
<h2 id="introduction">Introduction</h2>
<p>I was introduced to the idea &ldquo;Image to Image Translation&rdquo; by a Kaggle competition named <a href="https://www.kaggle.com/c/gan-getting-started"target="_blank" rel="noopener noreferrer">&ldquo;I&rsquo;m Something of a Painter Myself&rdquo;</a>
. The competition was about creating a GAN model that can translate a given set of digital photos to paintings. After going through almost every publicly available notebook in Kaggle and reading many different articles and the <a href="https://arxiv.org/abs/1703.10593"target="_blank" rel="noopener noreferrer">CycleGAN paper</a>
 I was eventually able to get some idea on CycleGANs.</p>
<h2 id="objective">Objective</h2>
<p>The model should be able to translate any given digital image photos to Monet like paintings. Some other examples will be conversion of a horse into zebra, day time images to night time images etc. See the figure below to get an idea.</p>
<p>Also model is trained on an unpaired datasets, which means that for any horse image present in the dataset a similar looking zebra image need not be present, which makes the problem even more complex.</p>
<h2 id="cyclegan-model">CycleGAN Model</h2>
<p>Generative Adversarial Networks were firstly introduced by Ian Goodfellow et.al[6], where they basically proposed an adversarial networks consisting of a Generator and a Discriminator. Generator as the name suggests, continuously generates output and the objective of the discriminator is to determine whether this output is real or fake. CycleGAN is basically following the idea but instead of using a single Generator and Discriminator, two of each is used. If we have two image domains X and Y (day and night), CycleGAN consists of a Generator G : X -&gt; Y, the second Generator F: Y -&gt; X and two Discriminator Dx and Dy. <br>
Firstly an image of either of the domain(say X) is taken and is fed into G that generates an image of Domain Y, this image is fed into both Dy and F. Dy classifies it as real or fake, thus optimizing G (Adversarial Loss), whereas F takes in the output of G as the input, and generates an image of domain X known as the reconstructed image, which should be the same as the original input image (Cycle Loss). We also pass an image of domain Y to G to calculate the identity loss. Similar process is repeated by taking in an image from domain Y as the input.<br>
<img  src="https://github.com/nivedwho/Images/blob/main/base.png?raw=true"
        alt="Base Model"/></p>
<p>The above diagram shows the architecture of the CycleGAN Model along with the three loss functions - Adversarial Loss, Content Loss and Identity Loss.</p>
<h4 id="adversarial-loss">Adversarial loss</h4>
<p>When the Generator G tries to translate input image from domain X to a similar looking image in domain Y, while the Discriminator Dy aims to distinguish images from both the domains. The authors of the original refers to this process as a minimax game where the Discriminator is trying to maximize the probability of correct classification and the Generator is trying to minimize the same. Adversarial loss encourages the generators to generate visually appealing images, which happens to be one of the biggest upside of using GANs.</p>
<h4 id="cycle-consistency-loss">Cycle Consistency loss</h4>
<p>Adversarial loss ensures that the Generator fools the descriminator for each generated image which does not really mean that the generated image is what we need. Therefore we use Cycle Consistency loss to ensure that random images are not generated. The Input image of domain X, when translated to domain Y and then back to X should output exactly the same image and Cycle Consistency loss is minimized to achieve the same.</p>
<h4 id="identity-loss">Identity loss</h4>
<p>Here an input image of domain Y is passed onto the Generator X-&gt;Y and an image known as identity image which is of the same domain Y. Obviously the Input and Output image in this case should be identical. This again ensures that generator does not generate random images.</p>
<h2 id="my-contributions">My Contributions</h2>
<h4 id="new-architecture">New Architecture</h4>
<p>One big disadvantage of CycleGAN is training them is really difficult and the training time is too long, thanks to its multiple generators and discriminators.So I eliminated the use of Discriminator Dy. This did not affect the output quality by much rather decreased the training time. The modified architecture is shown in the below diagram.<br>
<img  src="https://github.com/nivedwho/Images/blob/main/new.png?raw=true"
        alt="Modified Model"/></p>
<h4 id="improved-identity-loss">Improved Identity Loss</h4>
<p>Layer of pretrained VGG-19 Network was used for feature extraction and the loss was calculated on the basis of these features. This improved the quality of generated outputs.</p>
<h4 id="new-datasets">New Datasets</h4>
<p>Both the original and modified model was trained on Monet-Photos Dataset and a Day-Night image dataset, which I had created by combining different images from publicly available sources. Both the original and modified models were trained on these datasets and I was able to Translate photos to paintings, Night time images to Day time images. Additionally I also wanted to compare the two models using metrics such as MSE, PSNR and SSIM and therefore created a dataset with some random colored photos, then took another set of photos and converted them to grayscale.</p>
<h2 id="evaluation">Evaluation</h2>
<p>Both the models were then trained to translate a colored image to grayscale, and the generated image was compared with the grayscale version of the input image. I found that usage of VGG network gave better results compared to the original one, but there was some reduction in quality when I modified the architecture. Still any reduction in training time is always a good thing especially in the case deep networks such as CycleGANs.
<img  src="https://github.com/nivedwho/Images/blob/main/eval.png?raw=true"
        alt="Evaluation Table"/></p>
<h2 id="results">Results</h2>
<p>The below images shows a few of the results I obtained.
<img  src="https://github.com/nivedwho/Images/blob/main/photo.jpg?raw=true"
        alt="Monet"/>
<img  src="https://github.com/nivedwho/Images/blob/main/daynight.jpg?raw=true"
        alt="DayNight"/><br>
<img  src="https://github.com/nivedwho/Images/blob/main/nightday.jpg?raw=true"
        alt="DayNight"/></p>
<p>The model took me to the 4th place in the kaggle competition mentioned earlier, and the model with a single discriminator had almost the same score as the original one with considerably less training time.</p></article><section class="article labels"><a class="category" href=/blog/categories/vision/>Vision</a><a class="category" href=/blog/categories/deep-learning/>Deep Learning</a><a class="category" href=/blog/categories/gans/>GANs</a></section>
</div>
<div class="article bottom"><section class="article navigation"><p><a class="link" href="/blog/posts/singan/"><span class="iconfont icon-article"></span>SinGAN - A GAN that only needs a single image</a></p><p><a class="link" href="/blog/posts/srgan/"><span class="iconfont icon-article"></span>Using GANs for Single Image Super Resolution</a></p></section></div></section><section id="footer"><div class="footer-wrap">
    <p class="copyright">Nived's Blog</p>
    <p class="powerby"><span>Powered&nbsp;by&nbsp;</span><a href="https://gohugo.io" 
        target="_blank" rel="noopener noreferrer">Hugo</a><span>&nbsp;&amp;&nbsp;</span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank" rel="noopener noreferrer">Notepadium</a></p></div>
</section></body>

</html>