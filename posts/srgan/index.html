<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.80.0" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>Using GANs for Single Image Super Resolution&nbsp;&ndash;&nbsp;Nived&#39;s Blog</title><link rel="stylesheet" href="/blog/css/core.min.25016403f866c4082f80e205de1efeb560e52c93535070b96b466132683bb44487a0057d0aada20d478abc9733966aa1.css" integrity="sha384-JQFkA/hmxAgvgOIF3h7&#43;tWDlLJNTUHC5a0ZhMmg7tESHoAV9Cq2iDUeKvJczlmqh"><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Using GANs for Single Image Super Resolution" /><body><section id="header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/blog/"><span class="site name">Nived's Blog</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="https://supertramp2%2egithub%2eio/"target="_blank" rel="noopener noreferrer">My Website</a></nav></div></span></div></section><section id="content"><div class="article-container"><section class="article header">
    <h1 class="article title">Using GANs for Single Image Super Resolution</h1><p class="article date"> </p></section><article class="article markdown-body"><p>A project reproducing the results of the paper <a href="https://ieeexplore.ieee.org/document/8099502"target="_blank" rel="noopener noreferrer">&ldquo;Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network&rdquo;</a>
 and making some additional changes to improve its performance.</p>
<p><a href="https://colab.research.google.com/github/supertramp2/Colab/blob/main/SRGAN.ipynb"target="_blank" rel="noopener noreferrer"><img  src="https://colab.research.google.com/assets/colab-badge.svg"
        alt="Open in Colab"/></a>
</p>
<h2 id="introduction">Introduction</h2>
<p>Single image super resolution is the process of recovering high-resolution (HR) images from a single corresponding low-resolution (LR) image. It demands addition of missing information present in the image. It is a field where active research is taking place and in the past years there are several deep learning models proposed. Few of these models also use Generative Adversarial Networks and one such model is known as SRGAN. GANs consists of a generator and a discriminator. Generator as the name suggests, continuously generates an output which maps a noise z to the input and the objective of the discriminator is to determine whether this output is real or generated. This why the model is known as an adversarial network. In the case of single image super Resolution, the input to generator is a Low Resolution image and the generator outputs a High Resolution image which is then passed onto the Discriminator estimates the probability the image as a generated image or a real one. In other words, the primary aim of the generator is to fool the discriminator and this is what makes Generative Adversarial Networks generate visually appealing images unlike the previously developed Deep Learning approaches.</p>
<h2 id="objective">Objective</h2>
<p>The model should be able to increase the resolution of an input sample by 4 times.</p>
<h2 id="srgan-model">SRGAN Model</h2>
<p>RGAN consists of a Generator and a Discriminator, where the Generator takes in a low resolution image as input and output a high resolution image and the Discriminator takes in this high resolution image and classifies the image as real or generated. When Generative Adversarial Networks was introduced for the first time in, the authors of the paper drew an analogy comparing generators and discriminators with counterfeiters and police. The counterfeiters generate fake currencies and the aim of the police is to identify the fake ones and catch the counterfeiters, once caught the counterfeiters will try generating better currencies that will reduce the chances of them getting caught. Similarly in the case of GANs both the generator and the discriminator improves their performance with time and the loss function used for this purpose is known as the Adversarial loss function. \linebreak
The <a href="https://ieeexplore.ieee.org/document/8099502"target="_blank" rel="noopener noreferrer">base paper</a>
 also computes the content loss by comparing the generated high resolution image and the original high resolution image. Since the use of traditional methods such as Mean Square Error between the two images for calculating loss functions gave poor results, the authors used high level feature maps of the pretrained VGG 19 networks for extracting features and comparing the Generated and the Original Image. In the <a href="https://ieeexplore.ieee.org/document/8099502"target="_blank" rel="noopener noreferrer">base paper</a>
, SRGAN model was evaluated using various metrics including mean opinion scores and the model was able to achieve state of the art results. We additionally introduced a new loss function named Identity Loss functions that improved the performance of the generator for generating images with higher perceptual quality. For computing identity loss, during the training process we pass the HR image present in the dataset to the Generator and the output of this Generator is compared with the original image similar to how we computed the content loss. Ideally this generated image should be exactly the same as the input image.<br>
<img  src="https://github.com/supertramp2/Images/blob/main/block.png?raw=true"
        alt="Base Model"/></p>
<h2 id="architecture">Architecture</h2>
<p><img  src="https://github.com/supertramp2/Images/blob/main/model.png?raw=true"
        alt="Base Model"/></p>
<h2 id="loss-functions">Loss Functions</h2>
<h4 id="adversarial-loss">Adversarial loss</h4>
<p>When the Generator G tries to translate input image from domain X to a similar looking image in domain Y, while the Discriminator Dy aims to distinguish images from both the domains. The authors of the original refers to this process as a minimax game where the Discriminator is trying to maximize the probability of correct classification and the Generator is trying to minimize the same. Adversarial loss encourages the generators to generate visually appealing images, which happens to be one of the biggest upside of using GANs.</p>
<h4 id="content-loss">Content loss</h4>
<p>The  adversarial  loss  alone  cannot  produce  good  results.The  content  loss  compares  the  generated  HR  image  withthe original HR image. A number of deep learning methodshave used MSE loss between the two images but it fails toproduce  good  quality  results.  The  authors  of  the  paper have replaced the loss function by faking use of the feature maps of pretrained VGG-19 network to calculate the contentloss. During training time, the difference between the generated image and the original image present in the dataset is taken as the content loss.</p>
<h4 id="identity-loss">Identity loss</h4>
<p>I have introduced a new loss function that encourages the generator to output the same image when a HR image is inputted to it. This potentially helps the generator in recognizing the features and traits present in a high resolution image.</p>
<h2 id="results">Results</h2>
<p><img  src="https://github.com/supertramp2/blog/blob/main/Images/srgan_base.jpg?raw=true"
        alt="Base Model"/>
<img  src="https://github.com/supertramp2/Images/blob/main/celebA.jpg?raw=true"
        alt="Base Model"/>
<img  src="https://github.com/supertramp2/Images/blob/main/ourout.jpg?raw=true"
        alt="Base Model"/>
<img  src="https://github.com/supertramp2/Images/blob/main/newplot.png?raw=true"
        alt="Base Model"/></p></article><section class="article labels"><a class="category" href=/blog/categories/computer-vision/>Computer Vision</a><a class="category" href=/blog/categories/deep-learning/>Deep Learning</a><a class="category" href=/blog/categories/gan/>GAN</a></section>
</div>
<div class="article bottom"><section class="article navigation"><p><a class="link" href="/blog/posts/cyclegan/"><span class="iconfont icon-article"></span>Using GANs to create 'art'</a></p><p><a class="link" href="/blog/posts/handwriting/"><span class="iconfont icon-article"></span>First neural netowork on MNSIT dataset.</a></p></section></div></section><section id="footer"><div class="footer-wrap">
    <p class="copyright">Nived's Blog</p>
    <p class="powerby"><span>Powered&nbsp;by&nbsp;</span><a href="https://gohugo.io" 
        target="_blank" rel="noopener noreferrer">Hugo</a><span>&nbsp;&amp;&nbsp;</span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank" rel="noopener noreferrer">Notepadium</a></p></div>
</section></body>

</html>